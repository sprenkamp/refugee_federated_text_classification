{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kiliansprenkamp/miniconda3/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class distribution <class 'numpy.int64'> 1    168\n",
      "0    162\n",
      "4    160\n",
      "3    158\n",
      "2    118\n",
      "Name: y, dtype: int64\n",
      "running fed round 1 of Portugal with 59 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/kiliansprenkamp/miniconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:\n",
      "Training Loss: 1.591948103904724\n",
      "Validation Loss: 1.5862531065940857\n",
      "Validation Accuracy: 0.25\n",
      "Epoch 2/10:\n",
      "Training Loss: 1.4392409801483155\n",
      "Validation Loss: 1.578792691230774\n",
      "Validation Accuracy: 0.25\n",
      "Epoch 3/10:\n",
      "Training Loss: 1.3726386070251464\n",
      "Validation Loss: 1.646236538887024\n",
      "Validation Accuracy: 0.25\n",
      "running fed round 1 of France with 116 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:\n",
      "Training Loss: 1.6055569913652208\n",
      "Validation Loss: 1.5977102120717366\n",
      "Validation Accuracy: 0.13043478260869565\n",
      "Epoch 2/10:\n",
      "Training Loss: 1.550965083969964\n",
      "Validation Loss: 1.5976245403289795\n",
      "Validation Accuracy: 0.17391304347826086\n",
      "Epoch 3/10:\n",
      "Training Loss: 1.4257835679584079\n",
      "Validation Loss: 1.5392910639444988\n",
      "Validation Accuracy: 0.391304347826087\n",
      "running fed round 1 of Germany with 151 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:\n",
      "Training Loss: 1.6084809799989064\n",
      "Validation Loss: 1.5656737983226776\n",
      "Validation Accuracy: 0.43333333333333335\n",
      "Epoch 2/10:\n",
      "Training Loss: 1.5361307561397552\n",
      "Validation Loss: 1.536989450454712\n",
      "Validation Accuracy: 0.3333333333333333\n",
      "Epoch 3/10:\n",
      "Training Loss: 1.5657596190770466\n",
      "Validation Loss: 1.4750037491321564\n",
      "Validation Accuracy: 0.4\n",
      "running fed round 1 of Switzerland with 96 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:\n",
      "Training Loss: 1.6192587465047836\n",
      "Validation Loss: 1.6013012329737346\n",
      "Validation Accuracy: 0.15789473684210525\n",
      "Epoch 2/10:\n",
      "Training Loss: 1.5200439244508743\n",
      "Validation Loss: 1.4801735083262126\n",
      "Validation Accuracy: 0.42105263157894735\n",
      "Epoch 3/10:\n",
      "Training Loss: 1.334192469716072\n",
      "Validation Loss: 1.3743599653244019\n",
      "Validation Accuracy: 0.47368421052631576\n",
      "running fed round 1 of Belgium with 92 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:\n",
      "Training Loss: 1.582494122641427\n",
      "Validation Loss: 1.6622969309488933\n",
      "Validation Accuracy: 0.16666666666666666\n",
      "Epoch 2/10:\n",
      "Training Loss: 1.5125961984906877\n",
      "Validation Loss: 1.7959564526875813\n",
      "Validation Accuracy: 0.16666666666666666\n",
      "Epoch 3/10:\n",
      "Training Loss: 1.483601178441729\n",
      "Validation Loss: 1.763909896214803\n",
      "Validation Accuracy: 0.2222222222222222\n",
      "running fed round 1 of Spain with 142 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:\n",
      "Training Loss: 1.6264622536572544\n",
      "Validation Loss: 1.565717101097107\n",
      "Validation Accuracy: 0.25\n",
      "Epoch 2/10:\n",
      "Training Loss: 1.5515630895441228\n",
      "Validation Loss: 1.5291853845119476\n",
      "Validation Accuracy: 0.25\n",
      "Epoch 3/10:\n",
      "Training Loss: 1.3894118395718662\n",
      "Validation Loss: 1.4050131440162659\n",
      "Validation Accuracy: 0.5\n",
      "running fed round 1 of Poland with 62 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:\n",
      "Training Loss: 1.5819708585739136\n",
      "Validation Loss: 1.5994150042533875\n",
      "Validation Accuracy: 0.3333333333333333\n",
      "Epoch 2/10:\n",
      "Training Loss: 1.4771887540817261\n",
      "Validation Loss: 1.6113207936286926\n",
      "Validation Accuracy: 0.3333333333333333\n",
      "Epoch 3/10:\n",
      "Training Loss: 1.3795652389526367\n",
      "Validation Loss: 1.6789566278457642\n",
      "Validation Accuracy: 0.3333333333333333\n",
      "running fed round 1 of CzechRepublic with 48 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:\n",
      "Training Loss: 1.5931178629398346\n",
      "Validation Loss: 1.6008641123771667\n",
      "Validation Accuracy: 0.3\n",
      "Epoch 2/10:\n",
      "Training Loss: 1.5717360377311707\n",
      "Validation Loss: 1.6205008029937744\n",
      "Validation Accuracy: 0.3\n",
      "Epoch 3/10:\n",
      "Training Loss: 1.5632958710193634\n",
      "Validation Loss: 1.6309661865234375\n",
      "Validation Accuracy: 0.3\n",
      "running fed round 2 of Portugal with 59 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:\n",
      "Training Loss: 1.5367575883865356\n",
      "Validation Loss: 1.5930812358856201\n",
      "Validation Accuracy: 0.25\n",
      "Epoch 2/10:\n",
      "Training Loss: 1.4485288381576538\n",
      "Validation Loss: 1.6134250164031982\n",
      "Validation Accuracy: 0.25\n",
      "Epoch 3/10:\n",
      "Training Loss: 1.4187909364700317\n",
      "Validation Loss: 1.6017886996269226\n",
      "Validation Accuracy: 0.25\n",
      "running fed round 2 of France with 116 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:\n",
      "Training Loss: 1.5909649398591783\n",
      "Validation Loss: 1.6001903613408406\n",
      "Validation Accuracy: 0.13043478260869565\n",
      "Epoch 2/10:\n",
      "Training Loss: 1.5366308821572199\n",
      "Validation Loss: 1.5591785907745361\n",
      "Validation Accuracy: 0.13043478260869565\n",
      "Epoch 3/10:\n",
      "Training Loss: 1.4624752733442519\n",
      "Validation Loss: 1.5072204271952312\n",
      "Validation Accuracy: 0.2608695652173913\n",
      "running fed round 2 of Germany with 151 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:\n",
      "Training Loss: 1.557774970928828\n",
      "Validation Loss: 1.5326212644577026\n",
      "Validation Accuracy: 0.4\n",
      "Epoch 2/10:\n",
      "Training Loss: 1.4709281424681346\n",
      "Validation Loss: 1.449238359928131\n",
      "Validation Accuracy: 0.43333333333333335\n",
      "Epoch 3/10:\n",
      "Training Loss: 1.4093271891276042\n",
      "Validation Loss: 1.3829328417778015\n",
      "Validation Accuracy: 0.43333333333333335\n",
      "running fed round 2 of Switzerland with 96 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:\n",
      "Training Loss: 1.5859814435243607\n",
      "Validation Loss: 1.5485363801320393\n",
      "Validation Accuracy: 0.3157894736842105\n",
      "Epoch 2/10:\n",
      "Training Loss: 1.4736272394657135\n",
      "Validation Loss: 1.4885266224543254\n",
      "Validation Accuracy: 0.47368421052631576\n",
      "Epoch 3/10:\n",
      "Training Loss: 1.3865079134702682\n",
      "Validation Loss: 1.446683128674825\n",
      "Validation Accuracy: 0.3684210526315789\n",
      "running fed round 2 of Belgium with 92 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:\n",
      "Training Loss: 1.573267902646746\n",
      "Validation Loss: 1.7813080946604412\n",
      "Validation Accuracy: 0.16666666666666666\n",
      "Epoch 2/10:\n",
      "Training Loss: 1.572658998625619\n",
      "Validation Loss: 1.7998775641123455\n",
      "Validation Accuracy: 0.16666666666666666\n",
      "Epoch 3/10:\n",
      "Training Loss: 1.528589470045907\n",
      "Validation Loss: 1.7683531045913696\n",
      "Validation Accuracy: 0.16666666666666666\n",
      "running fed round 2 of Spain with 142 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:\n",
      "Training Loss: 1.5877887227318503\n",
      "Validation Loss: 1.5703174471855164\n",
      "Validation Accuracy: 0.39285714285714285\n",
      "Epoch 2/10:\n",
      "Training Loss: 1.4974236379970203\n",
      "Validation Loss: 1.5142960250377655\n",
      "Validation Accuracy: 0.42857142857142855\n",
      "Epoch 3/10:\n",
      "Training Loss: 1.3720678632909602\n",
      "Validation Loss: 1.4238625466823578\n",
      "Validation Accuracy: 0.5\n",
      "running fed round 2 of Poland with 62 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:\n",
      "Training Loss: 1.548956608772278\n",
      "Validation Loss: 1.5740201473236084\n",
      "Validation Accuracy: 0.4166666666666667\n",
      "Epoch 2/10:\n",
      "Training Loss: 1.416189432144165\n",
      "Validation Loss: 1.5546189546585083\n",
      "Validation Accuracy: 0.3333333333333333\n",
      "Epoch 3/10:\n",
      "Training Loss: 1.3551575183868407\n",
      "Validation Loss: 1.512394368648529\n",
      "Validation Accuracy: 0.5\n",
      "running fed round 2 of CzechRepublic with 48 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:\n",
      "Training Loss: 1.5903178453445435\n",
      "Validation Loss: 1.555391550064087\n",
      "Validation Accuracy: 0.4\n",
      "Epoch 2/10:\n",
      "Training Loss: 1.5580540299415588\n",
      "Validation Loss: 1.5205222964286804\n",
      "Validation Accuracy: 0.4\n",
      "Epoch 3/10:\n",
      "Training Loss: 1.4704530537128448\n",
      "Validation Loss: 1.5346968173980713\n",
      "Validation Accuracy: 0.4\n",
      "running fed round 3 of Portugal with 59 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:\n",
      "Training Loss: 1.4371025085449218\n",
      "Validation Loss: 1.4540022015571594\n",
      "Validation Accuracy: 0.3333333333333333\n",
      "Epoch 2/10:\n",
      "Training Loss: 1.3375170946121215\n",
      "Validation Loss: 1.3894118070602417\n",
      "Validation Accuracy: 0.5833333333333334\n",
      "Epoch 3/10:\n",
      "Training Loss: 1.2290229082107544\n",
      "Validation Loss: 1.4307671785354614\n",
      "Validation Accuracy: 0.4166666666666667\n",
      "running fed round 3 of France with 116 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:\n",
      "Training Loss: 1.4428170654508803\n",
      "Validation Loss: 1.431772510210673\n",
      "Validation Accuracy: 0.34782608695652173\n",
      "Epoch 2/10:\n",
      "Training Loss: 1.333155141936408\n",
      "Validation Loss: 1.382499059041341\n",
      "Validation Accuracy: 0.34782608695652173\n",
      "Epoch 3/10:\n",
      "Training Loss: 1.2378273142708673\n",
      "Validation Loss: 1.3153810898462932\n",
      "Validation Accuracy: 0.5217391304347826\n",
      "running fed round 3 of Germany with 151 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:\n",
      "Training Loss: 1.3819333414236705\n",
      "Validation Loss: 1.3176977634429932\n",
      "Validation Accuracy: 0.5\n",
      "Epoch 2/10:\n",
      "Training Loss: 1.2789764901002247\n",
      "Validation Loss: 1.2374654114246368\n",
      "Validation Accuracy: 0.5666666666666667\n",
      "Epoch 3/10:\n",
      "Training Loss: 1.1675847073396046\n",
      "Validation Loss: 1.2382726967334747\n",
      "Validation Accuracy: 0.4666666666666667\n",
      "running fed round 3 of Switzerland with 96 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:\n",
      "Training Loss: 1.513519898056984\n",
      "Validation Loss: 1.4821909666061401\n",
      "Validation Accuracy: 0.15789473684210525\n",
      "Epoch 2/10:\n",
      "Training Loss: 1.4023910760879517\n",
      "Validation Loss: 1.4675338665644329\n",
      "Validation Accuracy: 0.3157894736842105\n",
      "Epoch 3/10:\n",
      "Training Loss: 1.3825013786554337\n",
      "Validation Loss: 1.3278539180755615\n",
      "Validation Accuracy: 0.5789473684210527\n",
      "running fed round 3 of Belgium with 92 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:\n",
      "Training Loss: 1.4466326917920793\n",
      "Validation Loss: 1.5779861211776733\n",
      "Validation Accuracy: 0.2777777777777778\n",
      "Epoch 2/10:\n",
      "Training Loss: 1.377241781779698\n",
      "Validation Loss: 1.3138809998830159\n",
      "Validation Accuracy: 0.5555555555555556\n",
      "Epoch 3/10:\n",
      "Training Loss: 1.257302028792245\n",
      "Validation Loss: 1.4887422323226929\n",
      "Validation Accuracy: 0.3888888888888889\n",
      "running fed round 3 of Spain with 142 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:\n",
      "Training Loss: 1.438494617288763\n",
      "Validation Loss: 1.3969627618789673\n",
      "Validation Accuracy: 0.5714285714285714\n",
      "Epoch 2/10:\n",
      "Training Loss: 1.3489686142314563\n",
      "Validation Loss: 1.3409819304943085\n",
      "Validation Accuracy: 0.5357142857142857\n",
      "Epoch 3/10:\n",
      "Training Loss: 1.227274472063238\n",
      "Validation Loss: 1.446249783039093\n",
      "Validation Accuracy: 0.42857142857142855\n",
      "running fed round 3 of Poland with 62 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:\n",
      "Training Loss: 1.41477530002594\n",
      "Validation Loss: 1.5350279808044434\n",
      "Validation Accuracy: 0.4166666666666667\n",
      "Epoch 2/10:\n",
      "Training Loss: 1.3424928665161133\n",
      "Validation Loss: 1.3651237487792969\n",
      "Validation Accuracy: 0.5\n",
      "Epoch 3/10:\n",
      "Training Loss: 1.258006501197815\n",
      "Validation Loss: 1.3418800830841064\n",
      "Validation Accuracy: 0.5\n",
      "running fed round 3 of CzechRepublic with 48 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:\n",
      "Training Loss: 1.4281367361545563\n",
      "Validation Loss: 1.471983015537262\n",
      "Validation Accuracy: 0.3\n",
      "Epoch 2/10:\n",
      "Training Loss: 1.3831543624401093\n",
      "Validation Loss: 1.3411359190940857\n",
      "Validation Accuracy: 0.5\n",
      "Epoch 3/10:\n",
      "Training Loss: 1.4277520775794983\n",
      "Validation Loss: 1.436062514781952\n",
      "Validation Accuracy: 0.4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load df from CSV file\n",
    "data=\"df_shuffled_sven\"\n",
    "df = pd.read_csv(f'data/{data}.csv', on_bad_lines=\"skip\")\n",
    "# df = df[df[\"federation_level\"] == \"Germany\"]\n",
    "if data=='df_testing':\n",
    "    max_length = 512\n",
    "    df = df[df['x'].str.len() < max_length].sample(500)\n",
    "\n",
    "if data=='df_gpt_output':\n",
    "    max_length = 512\n",
    "    df = df[df['x'].str.len() < max_length]\n",
    "    new_class_dict={\"Medical\":np.int64(0), \n",
    "               \"Accommodation\":np.int64(1),\n",
    "               \"Government Services\":np.int64(2), \n",
    "               \"Banking\":np.int64(3), \n",
    "               \"Transport\":np.int64(4)}\n",
    "    df['y_gpt_pred_str'] = df['y_gpt_pred_str'].apply(lambda x: x.strip())\n",
    "    df['y'] = df['y_gpt_pred_str'].map(new_class_dict)\n",
    "    df = df.dropna(subset=['y'])\n",
    "\n",
    "if data=='df_shuffled_sven':\n",
    "    max_length = 512\n",
    "    df = df[df['x'].str.len() < max_length]\n",
    "    df = df[df['y']!=-1]\n",
    "    DF = df.dropna(subset=['y'])\n",
    "    # df = df[df.federation_level.isin([\"Germany\", \"France\", \"Spain\"])]\n",
    "print(\"class distribution\", type(df.y.iloc[0]), df.y.value_counts())\n",
    "model_name = 'bert-base-multilingual-uncased'\n",
    "num_labels = len(DF.y.unique())\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "with open(\"models/results/results_federated.txt\", \"w\") as file:\n",
    "    for fed_round in range(1,4):\n",
    "        weight_list = []\n",
    "        for country in DF.federation_level.unique():\n",
    "            df = DF[DF.federation_level==country]\n",
    "            print(f\"running fed round {fed_round} of {country} with {len(df)} \")\n",
    "            if fed_round==1:  \n",
    "                model = BertForSequenceClassification.from_pretrained(model_name, num_labels=num_labels) # NOTE num_labels need to start at 0 important for labelling\n",
    "                # Split the data into train, validation, and test sets\n",
    "            else:\n",
    "                model = BertForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "                # Set the model's weights to the averaged weights\n",
    "                model.load_state_dict(averaged_weights)\n",
    "\n",
    "            train_data, test_data = train_test_split(df, test_size=0.4, random_state=42)\n",
    "            val_data, test_data = train_test_split(test_data, test_size=0.5, random_state=42)\n",
    "\n",
    "            # Tokenize and encode the text data for train, validation, and test sets\n",
    "            train_inputs = tokenizer(\n",
    "                train_data['x'].tolist(),\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "\n",
    "            val_inputs = tokenizer(\n",
    "                val_data['x'].tolist(),\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "\n",
    "            test_inputs = tokenizer(\n",
    "                test_data['x'].tolist(),\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "\n",
    "            # Convert labels to tensors for train, validation, and test sets\n",
    "            train_labels = torch.tensor(train_data['y'].tolist())\n",
    "            val_labels = torch.tensor(val_data['y'].tolist())\n",
    "            test_labels = torch.tensor(test_data['y'].tolist())\n",
    "\n",
    "            # Create TensorDatasets for train, validation, and test sets\n",
    "            train_dataset = TensorDataset(train_inputs['input_ids'], train_inputs['attention_mask'], train_labels)\n",
    "            val_dataset = TensorDataset(val_inputs['input_ids'], val_inputs['attention_mask'], val_labels)\n",
    "            test_dataset = TensorDataset(test_inputs['input_ids'], test_inputs['attention_mask'], test_labels)\n",
    "\n",
    "            # Define batch size and create DataLoaders for train, validation, and test sets\n",
    "            batch_size = 8\n",
    "            train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "            val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "            test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "            # Define optimizer and learning rate scheduler\n",
    "            optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "            total_steps = len(train_dataloader) * 10  # 10 epochs\n",
    "            scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "            # Fine-tuning loop\n",
    "            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "            device = \"mps\"\n",
    "            # print(device)\n",
    "            model.to(device)\n",
    "\n",
    "            best_val_loss = float('inf')\n",
    "            best_model = None\n",
    "\n",
    "            for epoch in range(3):  # 10 epochs\n",
    "                print(f'Epoch {epoch + 1}/{10}:')\n",
    "                total_loss = 0\n",
    "                model.train()\n",
    "\n",
    "                for batch in train_dataloader:\n",
    "                    input_ids, attention_mask, batch_labels = batch\n",
    "                    input_ids = input_ids.to(device)\n",
    "                    attention_mask = attention_mask.to(device)\n",
    "                    batch_labels = batch_labels.to(device)\n",
    "\n",
    "                    model.zero_grad()\n",
    "\n",
    "                    outputs = model(\n",
    "                        input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        labels=batch_labels\n",
    "                    )\n",
    "\n",
    "                    loss = outputs.loss\n",
    "                    total_loss += loss.item()\n",
    "\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                    optimizer.step()\n",
    "\n",
    "                scheduler.step()\n",
    "\n",
    "                # Calculate average training loss for the epoch\n",
    "                avg_loss = total_loss / len(train_dataloader)\n",
    "                print(f'Training Loss: {avg_loss}')\n",
    "\n",
    "                # Evaluate on the validation set\n",
    "                model.eval()\n",
    "                val_loss = 0\n",
    "                val_correct = 0\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    for batch in val_dataloader:\n",
    "                        input_ids, attention_mask, batch_labels = batch\n",
    "                        input_ids = input_ids.to(device)\n",
    "                        attention_mask = attention_mask.to(device)\n",
    "                        batch_labels = batch_labels.to(device)\n",
    "\n",
    "                        outputs = model(\n",
    "                            input_ids=input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            labels=batch_labels\n",
    "                        )\n",
    "\n",
    "                        loss = outputs.loss\n",
    "                        logits = outputs.logits\n",
    "\n",
    "                        val_loss += loss.item()\n",
    "\n",
    "                        # Calculate the number of correct predictions\n",
    "                        _, predicted_labels = torch.max(logits, dim=1)\n",
    "                        val_correct += torch.sum(predicted_labels == batch_labels).item()\n",
    "\n",
    "                avg_val_loss = val_loss / len(val_dataloader)\n",
    "                val_accuracy = val_correct / len(val_dataset)\n",
    "\n",
    "                print(f'Validation Loss: {avg_val_loss}')\n",
    "                print(f'Validation Accuracy: {val_accuracy}')\n",
    "\n",
    "                # Save the best model based on validation loss\n",
    "                if avg_val_loss < best_val_loss:\n",
    "                    best_val_loss = avg_val_loss\n",
    "                    best_model = model.state_dict()\n",
    "\n",
    "            # Load the best model\n",
    "            model.load_state_dict(best_model)\n",
    "\n",
    "            # Evaluate the model on the test set\n",
    "            model.eval()\n",
    "            total_test_loss = 0\n",
    "            total_test_correct = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for batch in test_dataloader:\n",
    "                    input_ids, attention_mask, batch_labels = batch\n",
    "                    input_ids = input_ids.to(device)\n",
    "                    attention_mask = attention_mask.to(device)\n",
    "                    batch_labels = batch_labels.to(device)\n",
    "\n",
    "                    outputs = model(\n",
    "                        input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        labels=batch_labels\n",
    "                    )\n",
    "\n",
    "                    loss = outputs.loss\n",
    "                    logits = outputs.logits\n",
    "\n",
    "                    total_test_loss += loss.item()\n",
    "\n",
    "                    # Calculate the number of correct predictions\n",
    "                    _, predicted_labels = torch.max(logits, dim=1)\n",
    "                    total_test_correct += torch.sum(predicted_labels == batch_labels).item()\n",
    "\n",
    "            # Calculate average test loss and accuracy\n",
    "            avg_test_loss = total_test_loss / len(test_dataloader)\n",
    "            accuracy = total_test_correct / len(test_dataset)\n",
    "\n",
    "            # print(f'Test Loss: {avg_test_loss}')\n",
    "            file.write(f'Test Accuracy {fed_round} {country}: {accuracy}')\n",
    "            weight_list.append(model.state_dict())\n",
    "        # print(weight_list)\n",
    "        averaged_weights = {}\n",
    "\n",
    "        # Average the weights\n",
    "        for key in weight_list[0].keys():\n",
    "            weights_float = [weights[key].float() for weights in weight_list]\n",
    "            averaged_weights[key] = torch.mean(torch.stack(weights_float), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
